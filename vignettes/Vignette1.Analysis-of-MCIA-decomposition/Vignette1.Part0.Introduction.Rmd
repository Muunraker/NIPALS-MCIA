---
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amsfonts}
output:
  html_document: default
---

\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42) # NIPALS starts with a random vector
```

```{r, echo=FALSE, include=FALSE}
library(nipalsMCIA)
library(ggplot2)
library(ComplexHeatmap)
```

## Introduction 
In the mini-lectures we covered in detail the math behind MCIA as well as a generic pipeline. In this vignette we will cover the most important functions within the nipalsMCIA package as well as downstream analyses that help can interpret the MCIA decomposition. MCIA is applicable to any kind of multi-block data. For this vignette we are using a cancer data set from (Meng et al., 2016) that includes 21 subjects with three data blocks. The data blocks include mRNA levels (12895 features), microRNA levels (537 features) and protein levels (7016 features). Without a multi-block method, researchers may try to use feature reduction methods on each block individually but this strategy ignores relationships between different blocks or under-appreciates signals which are specific to one block. In the context of the NCI60 data set and this vignette, we will show you the power of MCIA to find important relationships between mRNA, microRNA and proteins. More specifically, we will show you how to interpret the global factor scores in Part 1 and global loadings in Part 2. 

**Preview of the NCI60 dataset**  

The NCI60 data set has been included with the nipalsMCIA package and is easily available as show below:
```{r}
# load the dataset, uses the name data_blocks
data(NCI60)
data_blocks$mrna[1:5,1:3]

data_blocks$miRNA[1:5,1:3]

data_blocks$prot[1:5,1:3]

```

**Running and Reviewing the MCIA output**  
We can compute the MCIA decomposition with the first 10 global factors by using: 
```{r, warning=FALSE, message=FALSE,}
mcia_results <- nipals_multiblock(data_blocks, preprocMethod='colprofile',
                                  num_PCs = 10, tol=1e-12, plots = 'none')
```

The results is a list of various decomposition matrices for MCIA and few other
values:
```{r}
names(mcia_results)
```

`global_scores` refers to $f^{(j)}$'s, `global_loadings` to $a^{(j)}$'s, `block_score_weights` to $?$, `block_scores` to $f_k^{(j)}$'s, `block_loadings` $a_k^{(j)}$'s, $eigvals$ to $?$, `preprocMethod` to the pre-processing method
used and `metadata` to any sample meta-data that has been passed. More details
on each of the matrices are covered as part of the math mini-lecture however we
will briefly review the `global_scores` and `global_loadings`.

The `global_scores` matrix is represented by $\mb F$, that is $n \times r$, where
$n$ is the number of samples and $r$ is the number of factors chosen by the
user with the `num_PCs = r` argument. Each column of this matrix represents one
of the orders of global factors computed, i.e.
$$
\mb F = \begin{pmatrix}
		| & |&  & |\\
		\mb f^{(1)} &\mb f^{(2)} & \hdots & \mb f^{(r)}\\
		| & |&  & |
	\end{pmatrix} \in \mathbb{R}^{n \times r}
$$
This matrix encodes a low-dimensional representation of the data set, with the
$i$-th row representing a set of $r$-dimensional coordinates for the $i$-th
sample. 

The `global_loadings` matrix is represented by $\mb A$ tht is $p \times r$, where
$p$ is the number of features across all omics and $r$ is as before. Each column
of this matrix represents one of the orders of global loadings computed, i.e.
$$
\mb A = \begin{pmatrix}
		| & |&  & |\\
		\mb a^{(1)} &\mb a^{(2)} & \hdots & \mb a^{(r)}\\
		| & |&  & |
	\end{pmatrix} \in \mathbb{R}^{p \times r}
$$
This matrix encodes the contribution of each feature to the  low-dimensional representation.

The remainder of this vignette will be broken down into two sections, **Part 1: Interpreting Global Factor Scores** and **Part 2: Interpreting Global
Loadings** where we show how to interpret $\mb F$ and $\mb A$, respectively. 